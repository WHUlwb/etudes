{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) formulates a hierachical model over collections of discrete data sets, for instance *words*, *documents* and *topics*. More specifically, LDA is a generative model over three hierarchies:\n",
    "\n",
    "1) every word of a document derives from a finite mixture of topics,\n",
    "2) every topic is derived from an possibly infinite set of topics,\n",
    "3) every document is prepresent by several topics.\n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol \\pi_i & \\sim \\text{Dirichlet}_K(\\alpha, \\dots, \\alpha) \\\\\n",
    "\\mathbf{b}_k & \\sim \\text{Dirichlet}_V(\\gamma, \\dots, \\gamma) \\\\\n",
    "q_{il} \\mid \\boldsymbol \\pi_i & \\sim \\text{Discrete}(\\boldsymbol \\pi_i) \\\\\n",
    "y_{il} \\mid q_{il} & \\sim \\text{Discrete}(\\mathbf{b}_k)\n",
    "\\end{align*}\n",
    "\n",
    "Here, we implement a Gibbs sample for inferring the distribution over the parameters following Kevin Murphy's [book](https://mitpress.mit.edu/books/machine-learning-1) and the [original paper](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) by David Blei. LDA is a hierarchical Bayesian model with the following model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:1:3: unexpected string constant\n4: Reference: Kevin Murphy's book Ch. 27\n5: \"\n     ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:1:3: unexpected string constant\n4: Reference: Kevin Murphy's book Ch. 27\n5: \"\n     ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LDA with Gibbs Sampler\n",
    "======================\n",
    "Reference: Kevin Murphy's book Ch. 27\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Words\n",
    "W = np.array([0, 1, 2, 3, 4])\n",
    "\n",
    "# D := document words\n",
    "X = np.array([\n",
    "    [0, 0, 1, 2, 2],\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [0, 1, 2, 2, 2],\n",
    "    [4, 4, 4, 4, 4],\n",
    "    [3, 3, 4, 4, 4],\n",
    "    [3, 4, 4, 4, 4]\n",
    "])\n",
    "\n",
    "N_D = X.shape[0]  # num of docs\n",
    "N_W = W.shape[0]  # num of words\n",
    "N_K = 2  # num of topics\n",
    "\n",
    "# Dirichlet priors\n",
    "alpha = 1\n",
    "gamma = 1\n",
    "\n",
    "\n",
    "# --------------\n",
    "# Initialization\n",
    "# --------------\n",
    "\n",
    "# Z := word topic assignment\n",
    "Z = np.zeros(shape=[N_D, N_W])\n",
    "for i in range(N_D):\n",
    "    for l in range(N_W):\n",
    "        Z[i, l] = np.random.randint(N_K)  # randomly assign word's topic\n",
    "\n",
    "# Pi := document topic distribution\n",
    "Pi = np.zeros([N_D, N_K])\n",
    "for i in range(N_D):\n",
    "    Pi[i] = np.random.dirichlet(alpha*np.ones(N_K))\n",
    "\n",
    "# B := word topic distribution\n",
    "B = np.zeros([N_K, N_W])\n",
    "for k in range(N_K):\n",
    "    B[k] = np.random.dirichlet(gamma*np.ones(N_W))\n",
    "\n",
    "\n",
    "# --------------\n",
    "# Gibbs sampling\n",
    "# --------------\n",
    "\n",
    "for it in range(1000):\n",
    "    # Sample from full conditional of Z\n",
    "    # ---------------------------------\n",
    "    for i in range(N_D):\n",
    "        for l in range(N_W):\n",
    "            # Calculate params for Z\n",
    "            p_bar_il = np.exp(np.log(Pi[i]) + np.log(B[:, X[i, l]]))\n",
    "            p_il = p_bar_il / np.sum(p_bar_il)\n",
    "\n",
    "            # Resample word topic assignment Z\n",
    "            z_il = np.random.multinomial(1, p_il)\n",
    "            Z[i, l] = np.argmax(z_il)\n",
    "\n",
    "    # Sample from full conditional of Pi\n",
    "    # ----------------------------------\n",
    "    for i in range(N_D):\n",
    "        m = np.zeros(N_K)\n",
    "\n",
    "        # Gather sufficient statistics\n",
    "        for k in range(N_K):\n",
    "            m[k] = np.sum(Z[i] == k)\n",
    "\n",
    "        # Resample doc topic dist.\n",
    "        Pi[i, :] = np.random.dirichlet(alpha + m)\n",
    "\n",
    "    # Sample from full conditional of B\n",
    "    # ---------------------------------\n",
    "    for k in range(N_K):\n",
    "        n = np.zeros(N_W)\n",
    "\n",
    "        # Gather sufficient statistics\n",
    "        for v in range(N_W):\n",
    "            for i in range(N_D):\n",
    "                for l in range(N_W):\n",
    "                    n[v] += (X[i, l] == v) and (Z[i, l] == k)\n",
    "\n",
    "        # Resample word topic dist.\n",
    "        B[k, :] = np.random.dirichlet(gamma + n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Kalman-filter\n",
    "\n",
    "This is an implementation of a Kalman-filter and the extendedn Kalman-filter as aprt if an exercise of *Recursive Estimation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "We model a discrete-time non-linear temporal system as:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{x}(k) &= q_{k-1}(\\mathbf{x}(k-1), \\mathbf{v}(k-1) ), \\\\\n",
    "    \\mathbf{y}(k) &= h_k(\\mathbf{x}(k), \\mathbf{w}(k)), \n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{x}_k$ are latent states, $\\mathbf{y}_k$ are  observations and $q$ and $h$ are (possibly non-linear) functions for states and observations at time $k \\in \\{0, 1, 2, \\dots \\}$.\n",
    "\n",
    "We estimate the latent states using *Bayesian tracking* which usually consists of two steps:\n",
    "\n",
    "* Prior update/state prediction: $f_p = f_{x(k) | z(k-1)} $\n",
    "* Posterior update/filtering: $f_m = f_{x(k) | z(k) } $\n",
    "\n",
    "For Gaussian error terms $\\mathbf{v} \\sim \\mathcal{N}(\\boldsymbol \\mu_v, \\boldsymbol \\Sigma_v)$, $\\mathbf{w} \\sim \\mathcal{N}(\\boldsymbol \\mu_w, \\boldsymbol \\Sigma_w)$ and *non-linear* $q$ and $h$, we can use an *extended Kalman filter* for estimation of the latent states, since then the state estimates are also Gaussian $\\mathbf{x}(k) = \\mathbf{x}_p(k) \\sim \\mathcal{N}(\\hat{\\boldsymbol \\mu}_p(k), \\hat{\\boldsymbol \\Sigma}_p(k))$.\n",
    "\n",
    "The main difference to a regular Kalman filter is that we first do a linear approximation of the system equations at the current state estimate and then apply the regular Kalman filter updates for means and covariances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load some `packages`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm, uniform, multivariate_normal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise describe the following non-linear system:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{x}(k) &= \\sin \\left( \\alpha \\mathbf{x}(k - 1) + \\alpha \\mathbf{v}(k-1)   \\right), \\\\ \n",
    "    \\mathbf{y}(k) &= \\cos \\left( \\beta \\mathbf{x}(k) \\right) + k \\mathbf{w}(k),\n",
    "\\end{align}\n",
    "\n",
    "where $\\alpha = 2 \\pi$, $\\beta = \\pi$, $\\mathbf{v} \\sim \\mathcal{N}(\\mathbf{0}, \\frac{1}{4}\\mathbf{I}) $ and $\\mathbf{w} \\sim \\mathcal{N}(\\mathbf{0}, \\frac{1}{4}\\mathbf{I}) $. The initial state is $\\mathbf{x}(0) = \\begin{pmatrix}1/12 \\\\  1/6\\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0    = np.array([1/12, 1/6])\n",
    "mu_m  = x0\n",
    "cov_m = np.matrix([[0, 0], [0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_v  = np.array([0, 0])\n",
    "cov_v = np.matrix([[1/4, 0], [0, 1/4]])\n",
    "mu_w  = np.array([0, 0])\n",
    "cov_w = np.matrix([[1/4, 0], [0, 1/4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 1\n",
    "alpha = 2 * np.pi\n",
    "beta  = np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_model(x, v, alpha):\n",
    "    su = np.add(np.multiply(alpha, x), np.multiply(alpha, v))\n",
    "    return np.sin(su)\n",
    "\n",
    "def observation_model(x, w, k, beta):\n",
    "    return np.add(np.cos(np.multiply(beta, x)), np.multiply(k, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Prior update\n",
    "\n",
    "**Assuming $\\mathbb{E}[\\mathbf{v}(kâˆ’1)] = \\mathbf{0} $** (otherwise we just substract the mean) we do a linear approximation of $q_{k-1}$ at $\\mathbf{x}(k)$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{x}(k) &  \\approx \\ q_{k-1}(\\hat{\\boldsymbol \\mu}_m(k-1), \\mathbf{0}) \n",
    "     +  \\mathbf{A}(k-1) (\\mathbf{x}(k-1) - \\hat{\\boldsymbol \\mu}_m(k-1)) \n",
    "     +  \\mathbf{L}(k-1) \\mathbf{v}(k-1) \\\\\n",
    "     & = \\mathbf{A}(k-1) \\mathbf{x}(k-1) + \\underbrace{ \\mathbf{L}(k-1) \\mathbf{v}(k-1) }_{ \\tilde{ \\mathbf{v} }(k-1) } + \\underbrace{ q_{k-1}(\\hat{\\boldsymbol \\mu}_m (k-1), \\mathbf{0}) - \\mathbf{A}(k-1) \\hat{\\boldsymbol \\mu}_m (k-1) }_{ \\boldsymbol \\xi(k-1) } \\\\\n",
    "     & = \\mathbf{A}(k-1) \\mathbf{x}(k-1) + \\tilde{\\mathbf{v}}(k-1) + \\boldsymbol \\xi(k-1),     \n",
    "\\end{align}\n",
    "\n",
    "where $ \\mathbf{A}(k-1) = \\frac{\\partial q_{k-1} (\\hat{\\boldsymbol \\mu}_m(k-1), \\mathbf{0}) }{\\partial \\mathbf{x}} $ and $ \\mathbf{L}(k-1) = \\frac{\\partial q_{k-1} (\\hat{\\boldsymbol \\mu}_m(k-1), \\mathbf{0}) }{\\partial \\mathbf{v}} $.\n",
    "\n",
    "The process noise has zero mean $\\mathbb{E}[\\mathbf{\\tilde{v}}(k-1)] = \\mathbf{0}$ and variance \n",
    "$\\mathbb{V}[ \\mathbf{\\tilde{v}}(k-1) ] = \\mathbf{L}(k-1) \\boldsymbol \\Sigma_v(k-1) \\mathbf{L}^T(k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in our example we have the following equations:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{A}(k) & = \\alpha  \\cos \\left( \\alpha \\mathbf{x}(k) + \\alpha \\mathbf{v}(k)   \\right) \\\\\n",
    "  \\mathbf{L}(k) & = \\alpha \\cos \\left( \\alpha \\mathbf{x}(k) + \\alpha \\mathbf{v}(k)  \\right) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def a_k(alpha, x, v):\n",
    "    su = np.multiply(alpha, x) + np.multiply(alpha, v)\n",
    "    return np.diag(np.multiply(alpha, np.cos(su)))\n",
    "\n",
    "def l_k(alpha, x, v):\n",
    "    su = np.multiply(alpha, x) + np.multiply(alpha, v)\n",
    "    return np.diag(np.multiply(alpha, np.cos(su)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.44139809  0.        ]\n",
      " [ 0.          3.14159265]]\n",
      "[[ 5.44139809  0.        ]\n",
      " [ 0.          3.14159265]]\n"
     ]
    }
   ],
   "source": [
    "A = a_k(alpha, mu_m, mu_v)\n",
    "print(A)\n",
    "L = l_k(alpha, mu_m, mu_v)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the prior update equation for the mean $\\hat{\\boldsymbol \\mu}_p(k)$ is:\n",
    "\n",
    "\\begin{align}\n",
    " \\hat{\\boldsymbol \\mu}_p(k) & = \\mathbf{A}(k-1) \\hat{\\boldsymbol \\mu}_m(k-1)  + \\boldsymbol \\xi(k-1) \\\\ \n",
    " & = q_{k-1}(\\hat{\\boldsymbol \\mu}_m(k-1), \\mathbf{0}).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_prior_mean(process_model, mu_m, v, alpha):\n",
    "    return process_model(mu_m, v, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5        0.8660254]\n"
     ]
    }
   ],
   "source": [
    "mu_p = update_prior_mean(process_model, mu_m, mu_v, alpha)\n",
    "print(mu_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the update for the prior variance $\\hat{\\boldsymbol \\Sigma}_p(k)$ is:\n",
    "\n",
    "\\begin{align}\n",
    " \\hat{\\boldsymbol \\Sigma}_p(k) & = \\mathbf{A}(k-1) \\hat{\\boldsymbol \\Sigma}_m(k-1) \\mathbf{A}^T(k-1) + \\mathbf{L}(k-1) \\boldsymbol \\Sigma_v(k-1) \\mathbf{L}^T(k-1).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_prior_covariance(cov_m, cov_v, A, L):\n",
    "    su1 = np.matmul(np.matmul(A, cov_m), A.T)\n",
    "    su2 = np.matmul(np.matmul(L, cov_v), L.T)\n",
    "    return np.add(su1, su2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.4022033  0.       ]\n",
      " [ 0.         2.4674011]]\n"
     ]
    }
   ],
   "source": [
    "cov_p = update_prior_covariance(cov_m, cov_v, A, L)\n",
    "print(cov_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior update\n",
    "\n",
    "We then do the posterior update by linearizing of $h_{k}$ at $\\mathbf{x}(k)$. **We assume zero mean $\\mathbb{E}[\\mathbf{w}(k)] = \\mathbf{0} $** for the measurement noise.\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{y}(k) &  \\approx \\ h_{k}(\\hat{\\boldsymbol \\mu}_p(k), \\mathbf{0}) \n",
    "     +  \\mathbf{B}(k) (\\mathbf{x}(k) - \\hat{\\boldsymbol \\mu}_p(k)) \n",
    "     +  \\mathbf{J}(k) \\mathbf{w}(k) \\\\\n",
    "     & = \\mathbf{B}(k)\\mathbf{x}(k) + \\underbrace{\\mathbf{J}(k)\\mathbf{w}(k)}_{\\tilde{\\mathbf{w}}(k)} + \n",
    "      \\underbrace{h_{k}(\\hat{\\boldsymbol \\mu}_p(k), \\mathbf{0})-  \\mathbf{B}(k)\\hat{\\boldsymbol \\mu}_p(k)}_{\\boldsymbol \\zeta(k)} \\\\\n",
    "      & = \\mathbf{B}(k)\\mathbf{x}(k)+ \\tilde{\\mathbf{w}}(k) + \\boldsymbol \\zeta(k),\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{B}(k) = \\frac{\\partial h_{k} (\\hat{\\boldsymbol \\mu}_p(k), \\mathbf{0}) }{\\partial \\mathbf{x}}$ and $\\mathbf{J}(k) = \\frac{\\partial h_{k} (\\hat{\\boldsymbol \\mu}_p(k), \\mathbf{0}) }{\\partial \\mathbf{w}}$.\n",
    "\n",
    "The measurement noise has zero mean $\\mathbb{E}[\\mathbf{\\tilde{w}}(k)] = \\mathbf{0}$ and variance \n",
    "$\\mathbb{V}[ \\mathbf{\\tilde{w}}(k) ] = \\mathbf{J}(k) \\boldsymbol \\Sigma_w(k) \\mathbf{J}^T(k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in our example we have the following equations:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{B}(k) & = - \\beta \\sin \\left( \\beta \\mathbf{x}(k) \\right) \\\\ \n",
    "  \\mathbf{J}(k) & = k\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def b_k(beta, x):\n",
    "    su = np.multiply(beta, x)\n",
    "    return np.diag(np.multiply(beta, -np.sin(su)))\n",
    "\n",
    "def j_k(k, dim):\n",
    "    return np.diag([k] * dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.14159265  0.        ]\n",
      " [ 0.         -1.28358009]]\n",
      "[[1 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "B = b_k(beta, mu_p)\n",
    "print(B)\n",
    "J = j_k(k, 2)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the Kalman gain matrix at time $k$ as:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{K}(k) &= \\hat{\\boldsymbol \\Sigma}_p(k) \\mathbf{B}^T(k) \\left( \\mathbf{B}(k) \\hat{\\boldsymbol \\Sigma}_p(k) \\mathbf{B}^T(k) + \\mathbf{J}(k) \\mathbf{\\Sigma}_w(k) \\mathbf{J}^T(k) \\right)^{-1}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_k(cov_p, cov_w, B, J):    \n",
    "    cov_p_b = np.matmul(cov_p, B.T)\n",
    "    cov_w_j = np.matmul(cov_w, J.T)\n",
    "    within = np.add(np.matmul(B, cov_p_b), np.matmul(J, cov_w_j))\n",
    "    return np.matmul(cov_p_b, np.linalg.inv(within))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.31722435  0.        ]\n",
      " [ 0.         -0.73393607]]\n"
     ]
    }
   ],
   "source": [
    "K = k_k(cov_p, cov_w, B, J)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then the update equation for the posterior mean $\\hat{\\boldsymbol \\mu}_m(k)$ is:\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{\\boldsymbol \\mu}_m(k) & = \\hat{\\boldsymbol \\mu}_p(k) + \\mathbf{K}(k) \\left( \\mathbf{y}(k) - \\mathbf{\\Upsilon}(k) \\hat{\\boldsymbol \\mu}_p(k) - \\boldsymbol \\zeta(k) \\right) \\\\\n",
    "     & = \\hat{\\boldsymbol \\mu}_p(k) + \\mathbf{K}(k) \\left( \\mathbf{y}(k) - h_{k}(\\hat{\\boldsymbol \\mu}_p(k), \\mathbf{0})\\right).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_posterior_mean(y, x, w, k, beta, observation_model, mu_p, K):\n",
    "    obs = observation_model(x, w, k, beta)\n",
    "    su = np.dot(K, np.subtract(y, obs) - 2)\n",
    "    return np.asarray(np.add(mu_p, su)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5         0.19614419]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([2, 2])\n",
    "mu_m = update_posterior_mean(y, mu_p, mu_w, k, beta, observation_model, mu_p, K)\n",
    "print(mu_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update for the posterior covariance $\\hat{\\boldsymbol \\Sigma}_m(k)$ is given by: \n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{\\boldsymbol \\Sigma}_m(k) &= \\left( \\mathbf{I} - \\mathbf{K}(k) \\mathbf{B}(k) \\right)  \\hat{\\boldsymbol \\Sigma}_p(k).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_posterior_covariance(K, B, cov_p):\n",
    "    I = np.diag([1] * K.shape[0])\n",
    "    su = np.subtract(I, np.matmul(K, B))\n",
    "    return np.matmul(su, cov_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02524391  0.        ]\n",
      " [ 0.          0.14294707]]\n"
     ]
    }
   ],
   "source": [
    "cov_m = update_posterior_covariance(K, B, cov_p)\n",
    "print(cov_m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

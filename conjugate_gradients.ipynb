{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugate gratients\n",
    "\n",
    "Here, we implement an iterative numerical solver, called *conjugate gradient algorithm*, to solve a system of linear equations\n",
    "\n",
    "\\begin{equation}\n",
    "Ax = b,\n",
    "\\end{equation}\n",
    "\n",
    "where $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive-definite matrix and $x$ and $b$ are real vectors of suitable sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's generate some data. Since $A$ needs to be postive definite (pd), we just sample a Wishart random variable which is guaranteed to be at least positive semi-definite (psd). If we then compute its square we get a pd matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83278163, 1.26179175, 0.51755842, 1.0452182 , 0.67888043],\n",
       "       [1.26179175, 3.84984806, 2.03011661, 2.12923287, 2.59537078],\n",
       "       [0.51755842, 2.03011661, 4.1467484 , 0.63641981, 2.26276269],\n",
       "       [1.0452182 , 2.12923287, 0.63641981, 1.53401186, 1.38406953],\n",
       "       [0.67888043, 2.59537078, 2.26276269, 1.38406953, 4.14337156]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "np.random.seed(23)\n",
    "\n",
    "A = stats.wishart.rvs(1, .5, size=(N, N), random_state=None)\n",
    "A = sp.dot(A.T, A)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if a matrix is pd by computing its eigen values. If all of them are greater zero the matrix is positive definite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix is pd:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrix is pd: \", sp.all(sp.linalg.eigvals(A) > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's generate a random vector $x$ which we try to estimate later. We use this in order to compute $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07132392, -0.04543758,  1.04088597, -0.09403473, -0.42084395])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sp.random.randn(N)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03799975,  0.55572826,  3.1750188 , -0.23558348,  0.31506669])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = A.dot(x)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part is the implentation of the conjugate gradient algorithm. It's is rather short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xk = sp.random.randn(N)\n",
    "rk = A.dot(xk) - b\n",
    "pk = -rk\n",
    "\n",
    "while any(rk > 0.000001):\n",
    "    alpha = rk.T.dot(rk) / pk.T.dot(A).dot(pk)\n",
    "    xk = xk + alpha * pk\n",
    "    rd = rk.T.dot(rk)\n",
    "    rk = rk + alpha * A.dot(pk)\n",
    "    beta = rk.T.dot(rk) / rd\n",
    "    pk = -rk + beta * pk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if our estimated $x$ vector is the same as the original $x$. Since we expect the solution to be unique, the two vectors should be almost identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07132392 -0.04543758  1.04088597 -0.09403473 -0.42084395]\n",
      "[-0.07132392 -0.04543758  1.04088597 -0.09403473 -0.42084395]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03799975  0.55572826  3.1750188  -0.23558348  0.31506669]\n",
      "[ 0.03799975  0.55572826  3.1750188  -0.23558348  0.31506669]\n"
     ]
    }
   ],
   "source": [
    "print(A.dot(x))\n",
    "print(A.dot(xk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, the two solutions are absolutely identical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-test",
   "language": "python",
   "name": "py36-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
